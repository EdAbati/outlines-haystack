{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤— `transformers`\n",
    "\n",
    "More info about using `transformers` models with outlines [here](https://dottxt-ai.github.io/outlines/latest/reference/models/transformers/)\n",
    "\n",
    "Table of Contents:\n",
    "- [JSON Generation](#json-generation)\n",
    "- [Text Generation](#text-generation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Generation\n",
    "\n",
    "Example based on https://dottxt-ai.github.io/outlines/latest/examples/extraction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T06:24:16.266963Z",
     "start_time": "2026-01-29T06:24:12.730836Z"
    }
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import jinja2\n",
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from outlines_haystack.generators.transformers import TransformersJSONGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T06:24:20.747102Z",
     "start_time": "2026-01-29T06:24:20.698603Z"
    }
   },
   "outputs": [],
   "source": [
    "class Pizza(str, Enum):\n",
    "    margherita = \"Margherita\"\n",
    "    calzone = \"Calzone\"\n",
    "\n",
    "\n",
    "class Order(BaseModel):\n",
    "    pizza: Pizza\n",
    "    number: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T06:24:23.005361Z",
     "start_time": "2026-01-29T06:24:22.994367Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are the owner of a pizza parlor. Customers\n",
    "send you orders from which you need to extract:\n",
    "\n",
    "1. The pizza that is ordered\n",
    "2. The number of pizzas\n",
    "\n",
    "# EXAMPLE\n",
    "\n",
    "ORDER: I would like one Margherita pizza\n",
    "RESULT: {\"pizza\": \"Margherita\", \"number\": 1}\n",
    "\n",
    "# OUTPUT INSTRUCTIONS\n",
    "\n",
    "Answer in valid JSON. Here are the different objects relevant for the output:\n",
    "\n",
    "Order:\n",
    "    pizza (str): name of the pizza\n",
    "    number (int): number of pizzas\n",
    "\n",
    "Return a valid JSON of type \"Order\"\n",
    "\n",
    "# OUTPUT\n",
    "\n",
    "ORDER: {{ order }}\n",
    "RESULT: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T06:24:32.333011Z",
     "start_time": "2026-01-29T06:24:32.281465Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = TransformersJSONGenerator(model_name=\"microsoft/Phi-3-mini-4k-instruct\", schema_object=Order, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use Jinja2 to render the template\n",
    "prompt = jinja2.Template(prompt_template).render(order=\"Is it possible to have 12 margheritas?\")\n",
    "generator.run(prompt=prompt, generation_kwargs={\"temperature\": 0.5, \"do_sample\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()\n",
    "pipeline.add_component(\n",
    "    instance=PromptBuilder(template=prompt_template, required_variables=[\"order\"]), name=\"prompt_builder\"\n",
    ")\n",
    "pipeline.add_component(\n",
    "    instance=TransformersJSONGenerator(\n",
    "        model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "        schema_object=Order,\n",
    "        device=device,\n",
    "    ),\n",
    "    name=\"llm\",\n",
    ")\n",
    "pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run(\n",
    "    {\n",
    "        \"prompt_builder\": {\"order\": \"Is it possible to have 12 margheritas?\"},\n",
    "        \"llm\": {\"generation_kwargs\": {\"temperature\": 0.5, \"do_sample\": True}},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "from outlines_haystack.generators.transformers import TransformersChoiceGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TransformersChoiceGenerator(\n",
    "    model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    choices=[\"Positive\", \"Negative\"],\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.run(\n",
    "    prompt=\"Classify the following statement: 'I love pizza'\", generation_kwargs={\"temperature\": 0.5, \"do_sample\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"Classify the following statement: '{{statement}}'\"\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(\n",
    "    instance=PromptBuilder(template=prompt_template, required_variables=[\"statement\"]), name=\"prompt_builder\"\n",
    ")\n",
    "pipeline.add_component(\n",
    "    instance=TransformersChoiceGenerator(\n",
    "        model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "        choices=[\"Positive\", \"Negative\"],\n",
    "    ),\n",
    "    name=\"llm\",\n",
    ")\n",
    "pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run(\n",
    "    {\n",
    "        \"prompt_builder\": {\"statement\": \"I love Italian food\"},\n",
    "        \"llm\": {\"generation_kwargs\": {\"temperature\": 0.5, \"do_sample\": True}},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "from outlines_haystack.generators.transformers import TransformersTextGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TransformersTextGenerator(\n",
    "    model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.run(prompt=\"What is the capital of Italy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"What is the capital of {{country}}?\"\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(\n",
    "    instance=PromptBuilder(template=prompt_template, required_variables=[\"country\"]), name=\"prompt_builder\"\n",
    ")\n",
    "pipeline.add_component(\n",
    "    instance=TransformersTextGenerator(\n",
    "        model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "        device=device,\n",
    "    ),\n",
    "    name=\"llm\",\n",
    ")\n",
    "pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run({\"prompt_builder\": {\"country\": \"France\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
