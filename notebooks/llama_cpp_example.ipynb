{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a49550be",
      "metadata": {},
      "source": [
        "# `llama.cpp`\n",
        "\n",
        "More info about using `llama.cpp` models with outlines [here](https://dottxt-ai.github.io/outlines/latest/reference/models/llama_cpp/)\n",
        "\n",
        "Table of Contents:\n",
        "- [JSON Generation](#json-generation)\n",
        "- [Choice Generation](#choice-generation)\n",
        "- [Text Generation](#text-generation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "098fc72c",
      "metadata": {},
      "source": [
        "## JSON Generation\n",
        "\n",
        "Example based on https://dottxt-ai.github.io/outlines/latest/cookbook/extraction/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bc1bba9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "import jinja2\n",
        "from haystack import Pipeline\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from outlines_haystack.generators.llama_cpp import LlamaCppJSONGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8c87101",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Pizza(str, Enum):\n",
        "    margherita = \"Margherita\"\n",
        "    calzone = \"Calzone\"\n",
        "\n",
        "\n",
        "class Order(BaseModel):\n",
        "    pizza: Pizza\n",
        "    number: int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e33e6fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"You are the owner of a pizza parlor. Customers \\\n",
        "send you orders from which you need to extract:\n",
        "\n",
        "1. The pizza that is ordered\n",
        "2. The number of pizzas\n",
        "\n",
        "# EXAMPLE\n",
        "\n",
        "ORDER: I would like one Margherita pizza\n",
        "RESULT: {\"pizza\": \"Margherita\", \"number\": 1}\n",
        "\n",
        "# OUTPUT INSTRUCTIONS\n",
        "\n",
        "Answer in valid JSON. Here are the different objects relevant for the output:\n",
        "\n",
        "Order:\n",
        "    pizza (str): name of the pizza\n",
        "    number (int): number of pizzas\n",
        "\n",
        "Return a valid JSON of type \"Order\"\n",
        "\n",
        "# OUTPUT\n",
        "\n",
        "ORDER: {{ order }}\n",
        "RESULT: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4602e2ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "generator = LlamaCppJSONGenerator(\n",
        "    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
        "    file_name=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
        "    schema_object=Order,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c2e059b",
      "metadata": {},
      "source": [
        "### Standalone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "161e84df",
      "metadata": {},
      "outputs": [],
      "source": [
        "generator.warm_up()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b27b204",
      "metadata": {},
      "outputs": [],
      "source": [
        "# we use Jinja2 to render the template\n",
        "prompt = jinja2.Template(prompt_template).render(order=\"Is it possible to have 12 margheritas?\")\n",
        "generator.run(prompt=prompt, generation_kwargs={\"temperature\": 0.5})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "900826fb",
      "metadata": {},
      "source": [
        "### In a Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e573524e",
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = Pipeline()\n",
        "pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
        "pipeline.add_component(\n",
        "    instance=LlamaCppJSONGenerator(\n",
        "        repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
        "        file_name=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
        "        schema_object=Order,\n",
        "    ),\n",
        "    name=\"llm\",\n",
        ")\n",
        "pipeline.connect(\"prompt_builder\", \"llm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be61af13",
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline.run(\n",
        "    {\n",
        "        \"prompt_builder\": {\"order\": \"Is it possible to have 12 margheritas?\"},\n",
        "        \"llm\": {\"generation_kwargs\": {\"temperature\": 0.5}},\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc112607",
      "metadata": {},
      "source": [
        "## Choice Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16942267",
      "metadata": {},
      "outputs": [],
      "source": [
        "from haystack import Pipeline\n",
        "from haystack.components.builders import PromptBuilder\n",
        "\n",
        "from outlines_haystack.generators.llama_cpp import LlamaCppChoiceGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80331e3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "generator = LlamaCppChoiceGenerator(\n",
        "    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
        "    file_name=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
        "    choices=[\"Positive\", \"Negative\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4bdb466",
      "metadata": {},
      "source": [
        "### Standalone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "313f708d",
      "metadata": {},
      "outputs": [],
      "source": [
        "generator.warm_up()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d1d6fb9",
      "metadata": {},
      "outputs": [],
      "source": [
        "generator.run(\n",
        "    prompt=\"Classify the following statement: 'I love pizza'\",\n",
        "    generation_kwargs={\"temperature\": 0.5},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "995b2e06",
      "metadata": {},
      "source": [
        "### In a Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f3c1653",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_template = \"Classify the following statement: '{{statement}}'\"\n",
        "\n",
        "pipeline = Pipeline()\n",
        "pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
        "pipeline.add_component(\n",
        "    instance=LlamaCppChoiceGenerator(\n",
        "        repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
        "        file_name=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
        "        choices=[\"Positive\", \"Negative\"],\n",
        "    ),\n",
        "    name=\"llm\",\n",
        ")\n",
        "pipeline.connect(\"prompt_builder\", \"llm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e8f7281",
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline.run(\n",
        "    {\n",
        "        \"prompt_builder\": {\"statement\": \"I love Italian food\"},\n",
        "        \"llm\": {\"generation_kwargs\": {\"temperature\": 0.5}},\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31e75e3f",
      "metadata": {},
      "source": [
        "## Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1176137b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from haystack import Pipeline\n",
        "from haystack.components.builders import PromptBuilder\n",
        "\n",
        "from outlines_haystack.generators.llama_cpp import LlamaCppTextGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fea4650",
      "metadata": {},
      "outputs": [],
      "source": [
        "generator = LlamaCppTextGenerator(\n",
        "    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
        "    file_name=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23d5d180",
      "metadata": {},
      "source": [
        "### Standalone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b1e7adf",
      "metadata": {},
      "outputs": [],
      "source": [
        "generator.warm_up()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f264bcec",
      "metadata": {},
      "outputs": [],
      "source": [
        "generator.run(\n",
        "    prompt=\"What is the capital of Italy?\",\n",
        "    generation_kwargs={\"temperature\": 0.5},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea13a8d7",
      "metadata": {},
      "source": [
        "### In a Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e6db0d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_template = \"What is the capital of {{country}}?\"\n",
        "\n",
        "pipeline = Pipeline()\n",
        "pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
        "pipeline.add_component(\n",
        "    instance=LlamaCppTextGenerator(\n",
        "        repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
        "        file_name=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
        "    ),\n",
        "    name=\"llm\",\n",
        ")\n",
        "pipeline.connect(\"prompt_builder\", \"llm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "260badc1",
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline.run(\n",
        "    {\n",
        "        \"prompt_builder\": {\"country\": \"France\"},\n",
        "        \"llm\": {\"generation_kwargs\": {\"temperature\": 0.5}},\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85eecac8",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.9)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
