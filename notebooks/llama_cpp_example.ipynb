{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a49550be",
      "metadata": {},
      "source": [
        "# `llama.cpp`\n",
        "\n",
        "More info about using `llama.cpp` models with outlines [here](https://dottxt-ai.github.io/outlines/latest/reference/models/llama_cpp/)\n",
        "\n",
        "Table of Contents:\n",
        "- [JSON Generation](#json-generation)\n",
        "- [Choice Generation](#choice-generation)\n",
        "- [Text Generation](#text-generation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "098fc72c",
      "metadata": {},
      "source": [
        "## JSON Generation\n",
        "\n",
        "Example based on https://dottxt-ai.github.io/outlines/latest/cookbook/extraction/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9bc1bba9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "import jinja2\n",
        "from haystack import Pipeline\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from outlines_haystack.generators.llama_cpp import LlamaCppJSONGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b8c87101",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Pizza(str, Enum):\n",
        "    margherita = \"Margherita\"\n",
        "    calzone = \"Calzone\"\n",
        "\n",
        "\n",
        "class Order(BaseModel):\n",
        "    pizza: Pizza\n",
        "    number: int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1e33e6fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"You are the owner of a pizza parlor. Customers \\\n",
        "send you orders from which you need to extract:\n",
        "\n",
        "1. The pizza that is ordered\n",
        "2. The number of pizzas\n",
        "\n",
        "# EXAMPLE\n",
        "\n",
        "ORDER: I would like one Margherita pizza\n",
        "RESULT: {\"pizza\": \"Margherita\", \"number\": 1}\n",
        "\n",
        "# OUTPUT INSTRUCTIONS\n",
        "\n",
        "Answer in valid JSON. Here are the different objects relevant for the output:\n",
        "\n",
        "Order:\n",
        "    pizza (str): name of the pizza\n",
        "    number (int): number of pizzas\n",
        "\n",
        "Return a valid JSON of type \"Order\"\n",
        "\n",
        "# OUTPUT\n",
        "\n",
        "ORDER: {{ order }}\n",
        "RESULT: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4602e2ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "generator = LlamaCppJSONGenerator(\n",
        "    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
        "    file_name=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
        "    schema_object=Order,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c2e059b",
      "metadata": {},
      "source": [
        "### Standalone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "161e84df",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/edoardoabati/projects/opensource/outlines-haystack/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `hf_hub_download`. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n",
            "llama_model_load_from_file_impl: using device Metal (Apple M1 Pro) - 25558 MiB free\n",
            "llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from /Users/edoardoabati/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct-gguf/snapshots/a64113399c2f6b8ad3e11c394733a2ddadaa7f33/./Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
            "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:   81 tensors\n",
            "llama_model_loader: - type q5_K:   32 tensors\n",
            "llama_model_loader: - type q6_K:   17 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 2.23 GiB (5.01 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control-looking token:  32007 '<|end|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
            "load: control-looking token:  32000 '<|endoftext|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: printing all EOG tokens:\n",
            "load:   - 32000 ('<|endoftext|>')\n",
            "load:   - 32007 ('<|end|>')\n",
            "load: special tokens cache size = 67\n",
            "load: token to piece cache size = 0.1690 MB\n",
            "print_info: arch             = phi3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 3072\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 96\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 96\n",
            "print_info: n_embd_head_v    = 96\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 3072\n",
            "print_info: n_embd_v_gqa     = 3072\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8192\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 3.82 B\n",
            "print_info: general.name     = Phi3\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32064\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 32000 '<|endoftext|>'\n",
            "print_info: EOT token        = 32007 '<|end|>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: PAD token        = 32000 '<|endoftext|>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 32000 '<|endoftext|>'\n",
            "print_info: EOG token        = 32007 '<|end|>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 194 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors: offloading 0 repeating layers to GPU\n",
            "load_tensors: offloaded 0/33 layers to GPU\n",
            "load_tensors:   CPU_Mapped model buffer size =  2281.66 MiB\n",
            "...........................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 512\n",
            "llama_context: n_ctx_per_seq = 512\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
            "ggml_metal_init: allocating\n",
            "ggml_metal_init: found device: Apple M1 Pro\n",
            "ggml_metal_init: picking default device: Apple M1 Pro\n",
            "ggml_metal_load_library: using embedded metal library\n",
            "ggml_metal_init: GPU name:   Apple M1 Pro\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyMetal4  (5002)\n",
            "ggml_metal_init: simdgroup reduction   = true\n",
            "ggml_metal_init: simdgroup matrix mul. = true\n",
            "ggml_metal_init: has residency sets    = true\n",
            "ggml_metal_init: has bfloat            = true\n",
            "ggml_metal_init: use bfloat            = false\n",
            "ggml_metal_init: hasUnifiedMemory      = true\n",
            "ggml_metal_init: recommendedMaxWorkingSetSize  = 26800.60 MB\n",
            "ggml_metal_init: loaded kernel_add                                    0x10179a530 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_2                             0x10179aec0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_3                             0x10179b850 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_4                             0x10179bce0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_5                             0x10179c170 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_6                             0x10179c4c0 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_7                             0x9ce448000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_8                             0x9ce448300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4                             0x9ce448600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x9ce448900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x9ce448c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x9ce448f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x9ce449200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x9ce449500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x9ce449800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x9ce449b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub                                    0x9ce449e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub_row_c4                             0x9ce44a100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul                                    0x9ce44a400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_row_c4                             0x9ce44a700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div                                    0x9ce44aa00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div_row_c4                             0x9ce44ad00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_id                                 0x9ce44b000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f32                             0x9ce44b300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f16                             0x9ce44b600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i32                             0x9ce44b900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i16                             0x9ce44bc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale                                  0x9ce298000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale_4                                0x9ce298300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_clamp                                  0x9ce298600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_tanh                                   0x9ce298900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_relu                                   0x9ce298c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sigmoid                                0x9ce298f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu                                   0x9ce299200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_4                                 0x9ce299500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_erf                               0x9ce299800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_erf_4                             0x9ce299b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick                             0x9ce299e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick_4                           0x9ce29a100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu                                   0x9ce29a400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu_4                                 0x9ce29a700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_elu                                    0x9ce29aa00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_abs                                    0x9ce29ad00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sgn                                    0x9ce29b000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_step                                   0x9ce29b300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_hardswish                              0x9ce29b600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_hardsigmoid                            0x9ce29b900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_exp                                    0x9ce29bc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16                           0x9ce2ac000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x9ce2ac300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32                           0x9ce2ac600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x9ce2ac900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf                          0x9ce2acc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x9ce2acf00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f32                           0x9ce2ad200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f16                           0x9ce2ad500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x9ce2ad800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x9ce2adb00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x9ce2ade00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x9ce2ae100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x9ce2ae400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x9ce2ae700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x9ce2aea00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x9ce2aed00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x9ce2af000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x9ce2af300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x9ce2af600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x9ce2af900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x9ce2afc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x9ce494000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x9ce494300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x9ce494600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x9ce494900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x9ce494c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x9ce494f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x9ce495200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_i32                           0x9ce495500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_f32                           0x9ce495800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_f16                           0x9ce495b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x9ce495e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x9ce496100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x9ce496400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x9ce496700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x9ce496a00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x9ce496d00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm                               0x9ce497000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm_mul                           0x9ce497300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x9ce497600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_l2_norm                                0x9ce497900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_group_norm                             0x9ce497c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_norm                                   0x9ce4ac000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x9ce4ac300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x9ce4ac600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x9ce4ac900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x9ce4acc00 | th_max =  384 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x9ce4acf00 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x9ce4ad200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x9ce4ad500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x9ce4ad800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x9ce4adb00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x9ce4ade00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x9ce4ae100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x9ce4ae400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x9ce4ae700 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x9ce4aea00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x9ce4aed00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x9ce4af000 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x9ce4af300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x9ce4af600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x9ce4af900 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x9ce4afc00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x9ce2d8000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x9ce2d8300 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x9ce2d8600 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x9ce2d8900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x9ce2d8c00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x9ce2d8f00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x9ce2d9200 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x9ce2d9500 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x9ce2d9800 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x9ce2d9b00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x9ce2d9e00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x9ce2da100 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x9ce2da400 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x9ce2da700 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x9ce2daa00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x9ce2dad00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x9ce2db000 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x9ce2db300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x9ce2db600 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x9ce2db900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x9ce2dbc00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x9ce2f0000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x9ce2f0300 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x9ce2f0600 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x9ce2f0900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x9ce2f0c00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x9ce2f0f00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x9ce2f1200 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x9ce2f1500 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x9ce2f1800 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x9ce2f1b00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x9ce2f1e00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x9ce2f2100 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x9ce2f2400 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x9ce2f2700 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x9ce2f2a00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x9ce2f2d00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x9ce2f3000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x9ce2f3300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x9ce2f3600 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x9ce2f3900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x9ce2f3c00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x9ce4e8000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x9ce4e8300 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x9ce4e8600 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x9ce4e8900 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x9ce4e8c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x9ce4e8f00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x9ce4e9200 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x9ce4e9500 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x9ce4e9800 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x9ce4e9b00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x9ce4e9e00 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x9ce4ea100 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x9ce4ea400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x9ce4ea700 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x9ce4eaa00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x9ce4ead00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x9ce4eb000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x9ce4eb300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x9ce4eb600 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x9ce4eb900 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x9ce4ebc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x9ce504000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x9ce504300 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x9ce504600 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x9ce504900 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x9ce504c00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x9ce504f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x9ce505200 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x9ce505500 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x9ce505800 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x9ce505b00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x9ce505e00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x9ce506100 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x9ce506400 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x9ce506700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x9ce506a00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x9ce506d00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x9ce507000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x9ce507300 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x9ce507600 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x9ce507900 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x9ce507c00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x9ce51c000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x9ce51c300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x9ce51c600 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x9ce51c900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x9ce51cc00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x9ce51cf00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x9ce51d200 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x9ce51d500 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x9ce51d800 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x9ce51db00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x9ce51de00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x9ce51e100 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x9ce51e400 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x9ce51e700 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x9ce51ea00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x9ce51ed00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x9ce51f000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x9ce51f300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x9ce51f600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x9ce51f900 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x9ce51fc00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x9ce35c000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x9ce35c300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x9ce35c600 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x9ce35c900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x9ce35cc00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x9ce35cf00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x9ce35d200 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x9ce35d500 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x9ce35d800 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x9ce35db00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x9ce35de00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x9ce35e100 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x9ce35e400 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x9ce35e700 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x9ce35ea00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x9ce35ed00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x9ce35f000 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x9ce35f300 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x9ce35f600 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x9ce35f900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f32                          0x9ce35fc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f16                          0x9ce374000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_multi_f32                         0x9ce374300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_multi_f16                         0x9ce374600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_vision_f32                        0x9ce374900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_vision_f16                        0x9ce374c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f32                          0x9ce374f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f16                          0x9ce375200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f16                             0x9ce375500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f32                             0x9ce375800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x9ce375b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x9ce375e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x9ce376100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x9ce376400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_upscale_f32                            0x9ce376700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_f32                                0x9ce376a00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x9ce376d00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x9ce377000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_arange_f32                             0x9ce377300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x9ce377600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x9ce377900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x9ce377c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x9ce380000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x9ce380300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x9ce380600 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x9ce380900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x9ce380c00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x9ce380f00 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x9ce381200 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x9ce381500 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x9ce381800 | th_max =  384 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x9ce381b00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x9ce381e00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x9ce382100 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x9ce382400 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x9ce382700 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x9ce382a00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x9ce382d00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x9ce383000 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x9ce383300 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x9ce383600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x9ce383900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x9ce383c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x9ce3a0000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x9ce3a0300 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x9ce3a0600 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x9ce3a0900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x9ce3a0c00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x9ce3a0f00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x9ce3a1200 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x9ce3a1500 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x9ce3a1800 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x9ce3a1b00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x9ce3a1e00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x9ce3a2100 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x9ce3a2400 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x9ce3a2700 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x9ce3a2a00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x9ce3a2d00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x9ce3a3000 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x9ce3a3300 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x9ce3a3600 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x9ce3a3900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x9ce3a3c00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x9ce3b8000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x9ce3b8300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x9ce3b8600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x9ce3b8900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x9ce3b8c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x9ce3b8f00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x9ce3b9200 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x9ce3b9500 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x9ce3b9800 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x9ce3b9b00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x9ce3b9e00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x9ce3ba100 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x9ce3ba400 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x9ce3ba700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x9ce3baa00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x9ce3bad00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x9ce3bb000 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x9ce3bb300 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x9ce3bb600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x9ce3bb900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x9ce3bbc00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x9ce594000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x9ce594300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x9ce594600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x9ce594900 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x9ce594c00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x9ce594f00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x9ce595200 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x9ce595500 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x9ce595800 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x9ce595b00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x9ce595e00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x9ce596100 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x9ce596400 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x9ce596700 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x9ce596a00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x9ce596d00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x9ce597000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x9ce597300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x9ce597600 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x9ce597900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x9ce597c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x9ce3e0000 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x9ce3e0300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x9ce3e0600 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x9ce3e0900 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x9ce3e0c00 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x9ce3e0f00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x9ce3e1200 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x9ce3e1500 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x9ce3e1800 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x9ce3e1b00 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x9ce3e1e00 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x9ce3e2100 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_f32                                0x9ce3e2400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_i32                                0x9ce3e2700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x9ce3e2a00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x9ce3e2d00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x9ce3e3000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x9ce3e3300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x9ce3e3600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x9ce3e3900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x9ce3e3c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x9ce5c8000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x9ce5c8300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x9ce5c8600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x9ce5c8900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x9ce5c8c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x9ce5c8f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x9ce5c9200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x9ce5c9500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x9ce5c9800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x9ce5c9b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x9ce5c9e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x9ce5ca100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x9ce5ca400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_concat                                 0x9ce5ca700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqr                                    0x9ce5caa00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqrt                                   0x9ce5cad00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sin                                    0x9ce5cb000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cos                                    0x9ce5cb300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_neg                                    0x9ce5cb600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_reglu                                  0x9ce5cb900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_geglu                                  0x9ce5cbc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_swiglu                                 0x9ce5e4000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_swiglu_oai                             0x9ce5e4300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_geglu_erf                              0x9ce5e4600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_geglu_quick                            0x9ce5e4900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sum_rows                               0x9ce5e4c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mean                                   0x9ce5e4f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argmax                                 0x9ce5e5200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x9ce5e5500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x9ce5e5800 | th_max = 1024 | th_width =   32\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "create_memory: n_ctx = 512 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified: layer  28: dev = CPU\n",
            "llama_kv_cache_unified: layer  29: dev = CPU\n",
            "llama_kv_cache_unified: layer  30: dev = CPU\n",
            "llama_kv_cache_unified: layer  31: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   192.00 MiB\n",
            "llama_kv_cache_unified: size =  192.00 MiB (   512 cells,  32 layers,  1/1 seqs), K (f16):   96.00 MiB, V (f16):   96.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 3\n",
            "llama_context: max_nodes = 1560\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =    73.01 MiB\n",
            "llama_context: graph nodes  = 1126\n",
            "llama_context: graph splits = 322 (with bs=512), 1 (with bs=1)\n",
            "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.pre': 'default', 'general.file_type': '15', 'general.quantization_version': '2', 'phi3.rope.dimension_count': '96', 'tokenizer.ggml.bos_token_id': '1', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.attention.head_count_kv': '32', 'phi3.attention.head_count': '32', 'tokenizer.ggml.model': 'llama', 'phi3.block_count': '32', 'general.architecture': 'phi3', 'phi3.feed_forward_length': '8192', 'phi3.embedding_length': '3072', 'general.name': 'Phi3', 'phi3.context_length': '4096'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
            "' + message['content'] + '<|end|>' + '\n",
            "' + '<|assistant|>' + '\n",
            "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
            "'}}{% endif %}{% endfor %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <s>\n"
          ]
        }
      ],
      "source": [
        "generator.warm_up()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4b27b204",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =    1988.72 ms\n",
            "llama_perf_context_print: prompt eval time =    1988.47 ms /   173 tokens (   11.49 ms per token,    87.00 tokens per second)\n",
            "llama_perf_context_print:        eval time =     541.04 ms /    15 runs   (   36.07 ms per token,    27.72 tokens per second)\n",
            "llama_perf_context_print:       total time =    2912.94 ms /   188 tokens\n",
            "llama_perf_context_print:    graphs reused =         14\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'structured_replies': ['{\"pizza\": \"Margherita\", \"number\": 12']}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we use Jinja2 to render the template\n",
        "prompt = jinja2.Template(prompt_template).render(order=\"Is it possible to have 12 margheritas?\")\n",
        "generator.run(prompt=prompt, generation_kwargs={\"temperature\": 0.5})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "900826fb",
      "metadata": {},
      "source": [
        "### In a Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e573524e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:haystack.components.builders.prompt_builder:PromptBuilder has 1 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<haystack.core.pipeline.pipeline.Pipeline object at 0x1270bce00>\n",
              " Components\n",
              "  - prompt_builder: PromptBuilder\n",
              "  - llm: LlamaCppJSONGenerator\n",
              " Connections\n",
              "  - prompt_builder.prompt -> llm.prompt (str)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline = Pipeline()\n",
        "pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
        "pipeline.add_component(\n",
        "    instance=LlamaCppJSONGenerator(\n",
        "        repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
        "        file_name=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
        "        schema_object=Order,\n",
        "    ),\n",
        "    name=\"llm\",\n",
        ")\n",
        "pipeline.connect(\"prompt_builder\", \"llm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "be61af13",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/edoardoabati/projects/opensource/outlines-haystack/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `hf_hub_download`. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n",
            "llama_model_load_from_file_impl: using device Metal (Apple M1 Pro) - 25553 MiB free\n",
            "llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from /Users/edoardoabati/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct-gguf/snapshots/a64113399c2f6b8ad3e11c394733a2ddadaa7f33/./Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
            "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:   81 tensors\n",
            "llama_model_loader: - type q5_K:   32 tensors\n",
            "llama_model_loader: - type q6_K:   17 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 2.23 GiB (5.01 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control-looking token:  32007 '<|end|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
            "load: control-looking token:  32000 '<|endoftext|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: printing all EOG tokens:\n",
            "load:   - 32000 ('<|endoftext|>')\n",
            "load:   - 32007 ('<|end|>')\n",
            "load: special tokens cache size = 67\n",
            "load: token to piece cache size = 0.1690 MB\n",
            "print_info: arch             = phi3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 3072\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 96\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 96\n",
            "print_info: n_embd_head_v    = 96\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 3072\n",
            "print_info: n_embd_v_gqa     = 3072\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8192\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 3.82 B\n",
            "print_info: general.name     = Phi3\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32064\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 32000 '<|endoftext|>'\n",
            "print_info: EOT token        = 32007 '<|end|>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: PAD token        = 32000 '<|endoftext|>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 32000 '<|endoftext|>'\n",
            "print_info: EOG token        = 32007 '<|end|>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 194 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors: offloading 0 repeating layers to GPU\n",
            "load_tensors: offloaded 0/33 layers to GPU\n",
            "load_tensors:   CPU_Mapped model buffer size =  2281.66 MiB\n",
            "...........................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 512\n",
            "llama_context: n_ctx_per_seq = 512\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
            "ggml_metal_init: allocating\n",
            "ggml_metal_init: found device: Apple M1 Pro\n",
            "ggml_metal_init: picking default device: Apple M1 Pro\n",
            "ggml_metal_init: GPU name:   Apple M1 Pro\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyMetal4  (5002)\n",
            "ggml_metal_init: simdgroup reduction   = true\n",
            "ggml_metal_init: simdgroup matrix mul. = true\n",
            "ggml_metal_init: has residency sets    = true\n",
            "ggml_metal_init: has bfloat            = true\n",
            "ggml_metal_init: use bfloat            = false\n",
            "ggml_metal_init: hasUnifiedMemory      = true\n",
            "ggml_metal_init: recommendedMaxWorkingSetSize  = 26800.60 MB\n",
            "ggml_metal_init: loaded kernel_add                                    0x9ce5e5b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_2                             0x9ce5e5e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_3                             0x9ce5e6100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_4                             0x9ce5e6400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_5                             0x9ce5e6700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_6                             0x9ce5e6a00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_7                             0x9ce5e6d00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_8                             0x9ce5e7000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4                             0x9ce5e7300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x9ce5e7600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x9ce5e7900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x9ce5e7c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x9cfd7c000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x9cfd7c300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x9cfd7c600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x9cfd7c900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub                                    0x9cfd7cc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub_row_c4                             0x9cfd7cf00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul                                    0x9cfd7d200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_row_c4                             0x9cfd7d500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div                                    0x9cfd7d800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div_row_c4                             0x9cfd7db00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_id                                 0x9cfd7de00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f32                             0x9cfd7e100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f16                             0x9cfd7e400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i32                             0x9cfd7e700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i16                             0x9cfd7ea00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale                                  0x9cfd7ed00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale_4                                0x9cfd7f000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_clamp                                  0x9cfd7f300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_tanh                                   0x9cfd7f600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_relu                                   0x9cfd7f900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sigmoid                                0x9cfd7fc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu                                   0x9cef68000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_4                                 0x9cef68300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_erf                               0x9cef68600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_erf_4                             0x9cef68900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick                             0x9cef68c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick_4                           0x9cef68f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu                                   0x9cef69200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu_4                                 0x9cef69500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_elu                                    0x9cef69800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_abs                                    0x9cef69b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sgn                                    0x9cef69e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_step                                   0x9cef6a100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_hardswish                              0x9cef6a400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_hardsigmoid                            0x9cef6a700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_exp                                    0x9cef6aa00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16                           0x9cef6ad00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x9cef6b000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32                           0x9cef6b300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x9cef6b600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf                          0x9cef6b900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x9cef6bc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f32                           0x9cef6c000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f16                           0x9cef6c300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x9cef6c600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x9cef6c900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x9cef6cc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x9cef6cf00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x9cef6d200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x9cef6d500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x9cef6d800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x9cef6db00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x9cef6de00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x9cef6e100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x9cef6e400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x9cef6e700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x9cef6ea00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x9cef6ed00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x9cef6f000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x9cef6f300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x9cef6f600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x9cef6f900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x9cef6fc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x9cfd80000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_i32                           0x9cfd80300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_f32                           0x9cfd80600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_f16                           0x9cfd80900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x9cfd80c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x9cfd80f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x9cfd81200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x9cfd81500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x9cfd81800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x9cfd81b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm                               0x9cfd81e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm_mul                           0x9cfd82100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x9cfd82400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_l2_norm                                0x9cfd82700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_group_norm                             0x9cfd82a00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_norm                                   0x9cfd82d00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x9cfd83000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x9cfd83300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x9cfd83600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x9cfd83900 | th_max =  384 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x9cfd83c00 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x9cefa8000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x9cefa8300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x9cefa8600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x9cefa8900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x9cefa8c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x9cefa8f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x9cefa9200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x9cefa9500 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x9cefa9800 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x9cefa9b00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x9cefa9e00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x9cefaa100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x9cefaa400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x9cefaa700 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x9cefaaa00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x9cefaad00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x9cefab000 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x9cefab300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x9cefab600 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x9cefab900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x9cefabc00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x9cefb8000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x9cefb8300 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x9cefb8600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x9cefb8900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x9cefb8c00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x9cefb8f00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x9cefb9200 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x9cefb9500 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x9cefb9800 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x9cefb9b00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x9cefb9e00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x9cefba100 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x9cefba400 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x9cefba700 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x9cefbaa00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x9cefbad00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x9cefbb000 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x9cefbb300 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x9cefbb600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x9cefbb900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x9cefbbc00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x9cfd90000 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x9cfd90300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x9cfd90600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x9cfd90900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x9cfd90c00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x9cfd90f00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x9cfd91200 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x9cfd91500 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x9cfd91800 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x9cfd91b00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x9cfd91e00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x9cfd92100 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x9cfd92400 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x9cfd92700 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x9cfd92a00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x9cfd92d00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x9cfd93000 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x9cfd93300 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x9cfd93600 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x9cfd93900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x9cfd93c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x9cefbc000 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x9cefbc300 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x9cefbc600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x9cefbc900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x9cefbcc00 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x9cefbcf00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x9cefbd200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x9cefbd500 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x9cefbd800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x9cefbdb00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x9cefbde00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x9cefbe100 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x9cefbe400 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x9cefbe700 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x9cefbea00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x9cefbed00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x9cefbf000 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x9cefbf300 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x9cefbf600 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x9cefbf900 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x9cefbfc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x9cfd94000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x9cfd94300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x9cfd94600 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x9cfd94900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x9cfd94c00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x9cfd94f00 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x9cfd95200 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x9cfd95500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x9cfd95800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x9cfd95b00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x9cfd95e00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x9cfd96100 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x9cfd96400 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x9cfd96700 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x9cfd96a00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x9cfd96d00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x9cfd97000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x9cfd97300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x9cfd97600 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x9cfd97900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x9cfd97c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x9cfd98000 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x9cfd98300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x9cfd98600 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x9cfd98900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x9cfd98c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x9cfd98f00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x9cfd99200 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x9cfd99500 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x9cfd99800 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x9cfd99b00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x9cfd99e00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x9cfd9a100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x9cfd9a400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x9cfd9a700 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x9cfd9aa00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x9cfd9ad00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x9cfd9b000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x9cfd9b300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x9cfd9b600 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x9cfd9b900 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x9cfd9bc00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x9cfd9c000 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x9cfd9c300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x9cfd9c600 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x9cfd9c900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x9cfd9cc00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x9cfd9cf00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x9cfd9d200 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x9cfd9d500 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x9cfd9d800 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x9cfd9db00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x9cfd9de00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x9cfd9e100 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x9cfd9e400 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x9cfd9e700 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f32                          0x9cfd9ea00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f16                          0x9cfd9ed00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_multi_f32                         0x9cfd9f000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_multi_f16                         0x9cfd9f300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_vision_f32                        0x9cfd9f600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_vision_f16                        0x9cfd9f900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f32                          0x9cfd9fc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f16                          0x9cefc0000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f16                             0x9cefc0300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f32                             0x9cefc0600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x9cefc0900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x9cefc0c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x9cefc0f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x9cefc1200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_upscale_f32                            0x9cefc1500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_f32                                0x9cefc1800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x9cefc1b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x9cefc1e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_arange_f32                             0x9cefc2100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x9cefc2400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x9cefc2700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x9cefc2a00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x9cefc2d00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x9cefc3000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x9cefc3300 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x9cefc3600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x9cefc3900 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x9cefc3c00 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x9cfda0000 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x9cfda0300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x9cfda0600 | th_max =  384 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x9cfda0900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x9cfda0c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x9cfda0f00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x9cfda1200 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x9cfda1500 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x9cfda1800 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x9cfda1b00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x9cfda1e00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x9cfda2100 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x9cfda2400 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x9cfda2700 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x9cfda2a00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x9cfda2d00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x9cfda3000 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x9cfda3300 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x9cfda3600 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x9cfda3900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x9cfda3c00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x9cefc4000 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x9cefc4300 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x9cefc4600 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x9cefc4900 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x9cefc4c00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x9cefc4f00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x9cefc5200 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x9cefc5500 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x9cefc5800 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x9cefc5b00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x9cefc5e00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x9cefc6100 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x9cefc6400 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x9cefc6700 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x9cefc6a00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x9cefc6d00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x9cefc7000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x9cefc7300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x9cefc7600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x9cefc7900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x9cefc7c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x9cfda4000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x9cfda4300 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x9cfda4600 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x9cfda4900 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x9cfda4c00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x9cfda4f00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x9cfda5200 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x9cfda5500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x9cfda5800 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x9cfda5b00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x9cfda5e00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x9cfda6100 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x9cfda6400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x9cfda6700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x9cfda6a00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x9cfda6d00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x9cfda7000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x9cfda7300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x9cfda7600 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x9cfda7900 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x9cfda7c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x9cfda8000 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x9cfda8300 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x9cfda8600 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x9cfda8900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x9cfda8c00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x9cfda8f00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x9cfda9200 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x9cfda9500 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x9cfda9800 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x9cfda9b00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x9cfda9e00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x9cfdaa100 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x9cfdaa400 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x9cfdaa700 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x9cfdaaa00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x9cfdaad00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x9cfdab000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x9cfdab300 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x9cfdab600 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x9cfdab900 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x9cfdabc00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x9cefc8000 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x9cefc8300 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x9cefc8600 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x9cefc8900 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x9cefc8c00 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x9cefc8f00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_f32                                0x9cefc9200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_i32                                0x9cefc9500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x9cefc9800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x9cefc9b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x9cefc9e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x9cefca100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x9cefca400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x9cefca700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x9cefcaa00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x9cefcad00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x9cefcb000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x9cefcb300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x9cefcb600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x9cefcb900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x9cefcbc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x9cefcc000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x9cefcc300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x9cefcc600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x9cefcc900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x9cefccc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x9cefccf00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x9cefcd200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_concat                                 0x9cefcd500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqr                                    0x9cefcd800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqrt                                   0x9cefcdb00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sin                                    0x9cefcde00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cos                                    0x9cefce100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_neg                                    0x9cefce400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_reglu                                  0x9cefce700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_geglu                                  0x9cefcea00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_swiglu                                 0x9cefced00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_swiglu_oai                             0x9cefcf000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_geglu_erf                              0x9cefcf300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_geglu_quick                            0x9cefcf600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sum_rows                               0x9cefcf900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mean                                   0x9cefcfc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argmax                                 0x9cfdac000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x9cfdac300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x9cfdac600 | th_max = 1024 | th_width =   32\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "create_memory: n_ctx = 512 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified: layer  28: dev = CPU\n",
            "llama_kv_cache_unified: layer  29: dev = CPU\n",
            "llama_kv_cache_unified: layer  30: dev = CPU\n",
            "llama_kv_cache_unified: layer  31: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   192.00 MiB\n",
            "llama_kv_cache_unified: size =  192.00 MiB (   512 cells,  32 layers,  1/1 seqs), K (f16):   96.00 MiB, V (f16):   96.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 3\n",
            "llama_context: max_nodes = 1560\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =    73.01 MiB\n",
            "llama_context: graph nodes  = 1126\n",
            "llama_context: graph splits = 322 (with bs=512), 1 (with bs=1)\n",
            "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.pre': 'default', 'general.file_type': '15', 'general.quantization_version': '2', 'phi3.rope.dimension_count': '96', 'tokenizer.ggml.bos_token_id': '1', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.attention.head_count_kv': '32', 'phi3.attention.head_count': '32', 'tokenizer.ggml.model': 'llama', 'phi3.block_count': '32', 'general.architecture': 'phi3', 'phi3.feed_forward_length': '8192', 'phi3.embedding_length': '3072', 'general.name': 'Phi3', 'phi3.context_length': '4096'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
            "' + message['content'] + '<|end|>' + '\n",
            "' + '<|assistant|>' + '\n",
            "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
            "'}}{% endif %}{% endfor %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <s>\n",
            "llama_perf_context_print:        load time =    1898.14 ms\n",
            "llama_perf_context_print: prompt eval time =    1897.88 ms /   173 tokens (   10.97 ms per token,    91.15 tokens per second)\n",
            "llama_perf_context_print:        eval time =     539.78 ms /    15 runs   (   35.99 ms per token,    27.79 tokens per second)\n",
            "llama_perf_context_print:       total time =    2441.27 ms /   188 tokens\n",
            "llama_perf_context_print:    graphs reused =         14\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'llm': {'structured_replies': ['{\"pizza\": \"Margherita\", \"number\": 12']}}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline.run(\n",
        "    {\n",
        "        \"prompt_builder\": {\"order\": \"Is it possible to have 12 margheritas?\"},\n",
        "        \"llm\": {\"generation_kwargs\": {\"temperature\": 0.5}},\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc112607",
      "metadata": {},
      "source": [
        "## Choice Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "16942267",
      "metadata": {},
      "outputs": [],
      "source": [
        "from haystack import Pipeline\n",
        "from haystack.components.builders import PromptBuilder\n",
        "\n",
        "from outlines_haystack.generators.llama_cpp import LlamaCppChoiceGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "80331e3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "generator = LlamaCppChoiceGenerator(\n",
        "    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
        "    file_name=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
        "    choices=[\"Positive\", \"Negative\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4bdb466",
      "metadata": {},
      "source": [
        "### Standalone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "313f708d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_load_from_file_impl: using device Metal (Apple M1 Pro) - 25553 MiB free\n",
            "llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from /Users/edoardoabati/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct-gguf/snapshots/a64113399c2f6b8ad3e11c394733a2ddadaa7f33/./Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
            "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:   81 tensors\n",
            "llama_model_loader: - type q5_K:   32 tensors\n",
            "llama_model_loader: - type q6_K:   17 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 2.23 GiB (5.01 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control-looking token:  32007 '<|end|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
            "load: control-looking token:  32000 '<|endoftext|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: printing all EOG tokens:\n",
            "load:   - 32000 ('<|endoftext|>')\n",
            "load:   - 32007 ('<|end|>')\n",
            "load: special tokens cache size = 67\n",
            "load: token to piece cache size = 0.1690 MB\n",
            "print_info: arch             = phi3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 3072\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 96\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 96\n",
            "print_info: n_embd_head_v    = 96\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 3072\n",
            "print_info: n_embd_v_gqa     = 3072\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8192\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 3.82 B\n",
            "print_info: general.name     = Phi3\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32064\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 32000 '<|endoftext|>'\n",
            "print_info: EOT token        = 32007 '<|end|>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: PAD token        = 32000 '<|endoftext|>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 32000 '<|endoftext|>'\n",
            "print_info: EOG token        = 32007 '<|end|>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 194 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors: offloading 0 repeating layers to GPU\n",
            "load_tensors: offloaded 0/33 layers to GPU\n",
            "load_tensors:   CPU_Mapped model buffer size =  2281.66 MiB\n",
            "...........................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 512\n",
            "llama_context: n_ctx_per_seq = 512\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
            "ggml_metal_init: allocating\n",
            "ggml_metal_init: found device: Apple M1 Pro\n",
            "ggml_metal_init: picking default device: Apple M1 Pro\n",
            "ggml_metal_init: GPU name:   Apple M1 Pro\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyMetal4  (5002)\n",
            "ggml_metal_init: simdgroup reduction   = true\n",
            "ggml_metal_init: simdgroup matrix mul. = true\n",
            "ggml_metal_init: has residency sets    = true\n",
            "ggml_metal_init: has bfloat            = true\n",
            "ggml_metal_init: use bfloat            = false\n",
            "ggml_metal_init: hasUnifiedMemory      = true\n",
            "ggml_metal_init: recommendedMaxWorkingSetSize  = 26800.60 MB\n",
            "ggml_metal_init: loaded kernel_add                                    0x9cfdac900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_2                             0x9cfdacc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_3                             0x9cfdacf00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_4                             0x9cfdad200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_5                             0x9cfdad500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_6                             0x9cfdad800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_7                             0x9cfdadb00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_8                             0x9cfdade00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4                             0x9cfdae100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x9cfdae400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x9cfdae700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x9cfdaea00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x9cfdaed00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x9cfdaf000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x9cfdaf300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x9cfdaf600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub                                    0x9cfdaf900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub_row_c4                             0x9cfdafc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul                                    0x9cfdcc000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_row_c4                             0x9cfdcc300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div                                    0x9cfdcc600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div_row_c4                             0x9cfdcc900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_id                                 0x9cfdccc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f32                             0x9cfdccf00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f16                             0x9cfdcd200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i32                             0x9cfdcd500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i16                             0x9cfdcd800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale                                  0x9cfdcdb00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale_4                                0x9cfdcde00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_clamp                                  0x9cfdce100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_tanh                                   0x9cfdce400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_relu                                   0x9cfdce700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sigmoid                                0x9cfdcea00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu                                   0x9cfdced00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_4                                 0x9cfdcf000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_erf                               0x9cfdcf300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_erf_4                             0x9cfdcf600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick                             0x9cfdcf900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick_4                           0x9cfdcfc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu                                   0x9d063c000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu_4                                 0x9d063c300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_elu                                    0x9d063c600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_abs                                    0x9d063c900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sgn                                    0x9d063cc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_step                                   0x9d063cf00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_hardswish                              0x9d063d200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_hardsigmoid                            0x9d063d500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_exp                                    0x9d063d800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16                           0x9d063db00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x9d063de00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32                           0x9d063e100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x9d063e400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf                          0x9d063e700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x9d063ea00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f32                           0x9d063ed00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f16                           0x9d063f000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x9d063f300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x9d063f600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x9d063f900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x9d063fc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x9d0640000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x9d0640300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x9d0640600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x9d0640900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x9d0640c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x9d0640f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x9d0641200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x9d0641500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x9d0641800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x9d0641b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x9d0641e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x9d0642100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x9d0642400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x9d0642700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x9d0642a00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x9d0642d00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_i32                           0x9d0643000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_f32                           0x9d0643300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_f16                           0x9d0643600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x9d0643900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x9d0643c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x9cfdd0000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x9cfdd0300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x9cfdd0600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x9cfdd0900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm                               0x9cfdd0c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm_mul                           0x9cfdd0f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x9cfdd1200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_l2_norm                                0x9cfdd1500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_group_norm                             0x9cfdd1800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_norm                                   0x9cfdd1b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x9cfdd1e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x9cfdd2100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x9cfdd2400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x9cfdd2700 | th_max =  384 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x9cfdd2a00 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x9cfdd2d00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x9cfdd3000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x9cfdd3300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x9cfdd3600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x9cfdd3900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x9cfdd3c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x9cfdd4000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x9cfdd4300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x9cfdd4600 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x9cfdd4900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x9cfdd4c00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x9cfdd4f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x9cfdd5200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x9cfdd5500 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x9cfdd5800 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x9cfdd5b00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x9cfdd5e00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x9cfdd6100 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x9cfdd6400 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x9cfdd6700 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x9cfdd6a00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x9cfdd6d00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x9cfdd7000 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x9cfdd7300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x9cfdd7600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x9cfdd7900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x9cfdd7c00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x9d0644000 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x9d0644300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x9d0644600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x9d0644900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x9d0644c00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x9d0644f00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x9d0645200 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x9d0645500 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x9d0645800 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x9d0645b00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x9d0645e00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x9d0646100 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x9d0646400 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x9d0646700 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x9d0646a00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x9d0646d00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x9d0647000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x9d0647300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x9d0647600 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x9d0647900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x9d0647c00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x9d0648000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x9d0648300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x9d0648600 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x9d0648900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x9d0648c00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x9d0648f00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x9d0649200 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x9d0649500 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x9d0649800 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x9d0649b00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x9d0649e00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x9d064a100 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x9d064a400 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x9d064a700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x9d064aa00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x9d064ad00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x9d064b000 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x9d064b300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x9d064b600 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x9d064b900 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x9d064bc00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x9d064c000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x9d064c300 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x9d064c600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x9d064c900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x9d064cc00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x9d064cf00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x9d064d200 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x9d064d500 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x9d064d800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x9d064db00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x9d064de00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x9d064e100 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x9d064e400 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x9d064e700 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x9d064ea00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x9d064ed00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x9d064f000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x9d064f300 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x9d064f600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x9d064f900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x9d064fc00 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x9cfdd8000 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x9cfdd8300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x9cfdd8600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x9cfdd8900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x9cfdd8c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x9cfdd8f00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x9cfdd9200 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x9cfdd9500 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x9cfdd9800 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x9cfdd9b00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x9cfdd9e00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x9cfdda100 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x9cfdda400 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x9cfdda700 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x9cfddaa00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x9cfddad00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x9cfddb000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x9cfddb300 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x9cfddb600 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x9cfddb900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x9cfddbc00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x9d0650000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x9d0650300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x9d0650600 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x9d0650900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x9d0650c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x9d0650f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x9d0651200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x9d0651500 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x9d0651800 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x9d0651b00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x9d0651e00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x9d0652100 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x9d0652400 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x9d0652700 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x9d0652a00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x9d0652d00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x9d0653000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x9d0653300 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x9d0653600 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x9d0653900 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x9d0653c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x9cfddc000 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x9cfddc300 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x9cfddc600 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x9cfddc900 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x9cfddcc00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x9cfddcf00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x9cfddd200 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x9cfddd500 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f32                          0x9cfddd800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f16                          0x9cfdddb00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_multi_f32                         0x9cfddde00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_multi_f16                         0x9cfdde100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_vision_f32                        0x9cfdde400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_vision_f16                        0x9cfdde700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f32                          0x9cfddea00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f16                          0x9cfdded00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f16                             0x9cfddf000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f32                             0x9cfddf300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x9cfddf600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x9cfddf900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x9cfddfc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x9cfde0000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_upscale_f32                            0x9cfde0300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_f32                                0x9cfde0600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x9cfde0900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x9cfde0c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_arange_f32                             0x9cfde0f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x9cfde1200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x9cfde1500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x9cfde1800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x9cfde1b00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x9cfde1e00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x9cfde2100 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x9cfde2400 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x9cfde2700 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x9cfde2a00 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x9cfde2d00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x9cfde3000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x9cfde3300 | th_max =  384 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x9cfde3600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x9cfde3900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x9cfde3c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x9cfde4000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x9cfde4300 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x9cfde4600 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x9cfde4900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x9cfde4c00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x9cfde4f00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x9cfde5200 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x9cfde5500 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x9cfde5800 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x9cfde5b00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x9cfde5e00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x9cfde6100 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x9cfde6400 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x9cfde6700 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x9cfde6a00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x9cfde6d00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x9cfde7000 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x9cfde7300 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x9cfde7600 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x9cfde7900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x9cfde7c00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x9cfde8000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x9cfde8300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x9cfde8600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x9cfde8900 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x9cfde8c00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x9cfde8f00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x9cfde9200 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x9cfde9500 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x9cfde9800 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x9cfde9b00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x9cfde9e00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x9cfdea100 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x9cfdea400 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x9cfdea700 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x9cfdeaa00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x9cfdead00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x9cfdeb000 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x9cfdeb300 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x9cfdeb600 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x9cfdeb900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x9cfdebc00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x9d0654000 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x9d0654300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x9d0654600 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x9d0654900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x9d0654c00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x9d0654f00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x9d0655200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x9d0655500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x9d0655800 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x9d0655b00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x9d0655e00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x9d0656100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x9d0656400 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x9d0656700 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x9d0656a00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x9d0656d00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x9d0657000 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x9d0657300 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x9d0657600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x9d0657900 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x9d0657c00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x9d0658000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x9d0658300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x9d0658600 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x9d0658900 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x9d0658c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x9d0658f00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x9d0659200 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x9d0659500 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x9d0659800 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x9d0659b00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x9d0659e00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x9d065a100 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x9d065a400 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x9d065a700 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x9d065aa00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x9d065ad00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x9d065b000 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x9d065b300 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x9d065b600 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x9d065b900 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x9d065bc00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_f32                                0x9cbc2c000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_i32                                0x9cbc2c300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x9cbc2c600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x9cbc2c900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x9cbc2cc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x9cbc2cf00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x9cbc2d200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x9cbc2d500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x9cbc2d800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x9cbc2db00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x9cbc2de00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x9cbc2e100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x9cbc2e400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x9cbc2e700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x9cbc2ea00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x9cbc2ed00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x9cbc2f000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x9cbc2f300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x9cbc2f600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x9cbc2f900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x9cbc2fc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x9cfdec000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_concat                                 0x9cfdec300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqr                                    0x9cfdec600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqrt                                   0x9cfdec900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sin                                    0x9cfdecc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cos                                    0x9cfdecf00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_neg                                    0x9cfded200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_reglu                                  0x9cfded500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_geglu                                  0x9cfded800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_swiglu                                 0x9cfdedb00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_swiglu_oai                             0x9cfdede00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_geglu_erf                              0x9cfdee100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_geglu_quick                            0x9cfdee400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sum_rows                               0x9cfdee700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mean                                   0x9cfdeea00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argmax                                 0x9cfdeed00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x9cfdef000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x9cfdef300 | th_max = 1024 | th_width =   32\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "create_memory: n_ctx = 512 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified: layer  28: dev = CPU\n",
            "llama_kv_cache_unified: layer  29: dev = CPU\n",
            "llama_kv_cache_unified: layer  30: dev = CPU\n",
            "llama_kv_cache_unified: layer  31: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   192.00 MiB\n",
            "llama_kv_cache_unified: size =  192.00 MiB (   512 cells,  32 layers,  1/1 seqs), K (f16):   96.00 MiB, V (f16):   96.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 3\n",
            "llama_context: max_nodes = 1560\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =    73.01 MiB\n",
            "llama_context: graph nodes  = 1126\n",
            "llama_context: graph splits = 322 (with bs=512), 1 (with bs=1)\n",
            "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.pre': 'default', 'general.file_type': '15', 'general.quantization_version': '2', 'phi3.rope.dimension_count': '96', 'tokenizer.ggml.bos_token_id': '1', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.attention.head_count_kv': '32', 'phi3.attention.head_count': '32', 'tokenizer.ggml.model': 'llama', 'phi3.block_count': '32', 'general.architecture': 'phi3', 'phi3.feed_forward_length': '8192', 'phi3.embedding_length': '3072', 'general.name': 'Phi3', 'phi3.context_length': '4096'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
            "' + message['content'] + '<|end|>' + '\n",
            "' + '<|assistant|>' + '\n",
            "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
            "'}}{% endif %}{% endfor %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <s>\n"
          ]
        }
      ],
      "source": [
        "generator.warm_up()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0d1d6fb9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =     434.20 ms\n",
            "llama_perf_context_print: prompt eval time =     433.99 ms /    13 tokens (   33.38 ms per token,    29.95 tokens per second)\n",
            "llama_perf_context_print:        eval time =      68.97 ms /     2 runs   (   34.48 ms per token,    29.00 tokens per second)\n",
            "llama_perf_context_print:       total time =     503.85 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'choice': 'Positive'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator.run(\n",
        "    prompt=\"Classify the following statement: 'I love pizza'\",\n",
        "    generation_kwargs={\"temperature\": 0.5},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "995b2e06",
      "metadata": {},
      "source": [
        "### In a Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4f3c1653",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:haystack.components.builders.prompt_builder:PromptBuilder has 1 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<haystack.core.pipeline.pipeline.Pipeline object at 0x1272d00b0>\n",
              " Components\n",
              "  - prompt_builder: PromptBuilder\n",
              "  - llm: LlamaCppChoiceGenerator\n",
              " Connections\n",
              "  - prompt_builder.prompt -> llm.prompt (str)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_template = \"Classify the following statement: '{{statement}}'\"\n",
        "\n",
        "pipeline = Pipeline()\n",
        "pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
        "pipeline.add_component(\n",
        "    instance=LlamaCppChoiceGenerator(\n",
        "        repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
        "        file_name=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
        "        choices=[\"Positive\", \"Negative\"],\n",
        "    ),\n",
        "    name=\"llm\",\n",
        ")\n",
        "pipeline.connect(\"prompt_builder\", \"llm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9e8f7281",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_load_from_file_impl: using device Metal (Apple M1 Pro) - 25553 MiB free\n",
            "llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from /Users/edoardoabati/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct-gguf/snapshots/a64113399c2f6b8ad3e11c394733a2ddadaa7f33/./Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
            "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:   81 tensors\n",
            "llama_model_loader: - type q5_K:   32 tensors\n",
            "llama_model_loader: - type q6_K:   17 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 2.23 GiB (5.01 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control-looking token:  32007 '<|end|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
            "load: control-looking token:  32000 '<|endoftext|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: printing all EOG tokens:\n",
            "load:   - 32000 ('<|endoftext|>')\n",
            "load:   - 32007 ('<|end|>')\n",
            "load: special tokens cache size = 67\n",
            "load: token to piece cache size = 0.1690 MB\n",
            "print_info: arch             = phi3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 3072\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 96\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 96\n",
            "print_info: n_embd_head_v    = 96\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 3072\n",
            "print_info: n_embd_v_gqa     = 3072\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8192\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 3.82 B\n",
            "print_info: general.name     = Phi3\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32064\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 32000 '<|endoftext|>'\n",
            "print_info: EOT token        = 32007 '<|end|>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: PAD token        = 32000 '<|endoftext|>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 32000 '<|endoftext|>'\n",
            "print_info: EOG token        = 32007 '<|end|>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 194 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors: offloading 0 repeating layers to GPU\n",
            "load_tensors: offloaded 0/33 layers to GPU\n",
            "load_tensors:   CPU_Mapped model buffer size =  2281.66 MiB\n",
            "...........................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 512\n",
            "llama_context: n_ctx_per_seq = 512\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
            "ggml_metal_init: allocating\n",
            "ggml_metal_init: found device: Apple M1 Pro\n",
            "ggml_metal_init: picking default device: Apple M1 Pro\n",
            "ggml_metal_init: GPU name:   Apple M1 Pro\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyMetal4  (5002)\n",
            "ggml_metal_init: simdgroup reduction   = true\n",
            "ggml_metal_init: simdgroup matrix mul. = true\n",
            "ggml_metal_init: has residency sets    = true\n",
            "ggml_metal_init: has bfloat            = true\n",
            "ggml_metal_init: use bfloat            = false\n",
            "ggml_metal_init: hasUnifiedMemory      = true\n",
            "ggml_metal_init: recommendedMaxWorkingSetSize  = 26800.60 MB\n",
            "ggml_metal_init: loaded kernel_add                                    0x9cfdef600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_2                             0x9cfdef900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_3                             0x9cfdefc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_4                             0x9cfdf8000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_5                             0x9cfdf8300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_6                             0x9cfdf8600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_7                             0x9cfdf8900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_8                             0x9cfdf8c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4                             0x9cfdf8f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x9cfdf9200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x9cfdf9500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x9cfdf9800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x9cfdf9b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x9cfdf9e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x9cfdfa100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x9cfdfa400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub                                    0x9cfdfa700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub_row_c4                             0x9cfdfaa00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul                                    0x9cfdfad00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_row_c4                             0x9cfdfb000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div                                    0x9cfdfb300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div_row_c4                             0x9cfdfb600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_id                                 0x9cfdfb900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f32                             0x9cfdfbc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f16                             0x9cfdfc000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i32                             0x9cfdfc300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i16                             0x9cfdfc600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale                                  0x9cfdfc900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale_4                                0x9cfdfcc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_clamp                                  0x9cfdfcf00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_tanh                                   0x9cfdfd200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_relu                                   0x9cfdfd500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sigmoid                                0x9cfdfd800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu                                   0x9cfdfdb00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_4                                 0x9cfdfde00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_erf                               0x9cfdfe100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_erf_4                             0x9cfdfe400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick                             0x9cfdfe700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick_4                           0x9cfdfea00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu                                   0x9cfdfed00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu_4                                 0x9cfdff000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_elu                                    0x9cfdff300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_abs                                    0x9cfdff600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sgn                                    0x9cfdff900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_step                                   0x9cfdffc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_hardswish                              0x9d0cd8000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_hardsigmoid                            0x9d0cd8300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_exp                                    0x9d0cd8600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16                           0x9d0cd8900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x9d0cd8c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32                           0x9d0cd8f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x9d0cd9200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf                          0x9d0cd9500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x9d0cd9800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f32                           0x9d0cd9b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f16                           0x9d0cd9e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x9d0cda100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x9d0cda400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x9d0cda700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x9d0cdaa00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x9d0cdad00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x9d0cdb000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x9d0cdb300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x9d0cdb600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x9d0cdb900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x9d0cdbc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x9d0cdc000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x9d0cdc300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x9d0cdc600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x9d0cdc900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x9d0cdcc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x9d0cdcf00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x9d0cdd200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x9d0cdd500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x9d0cdd800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x9d0cddb00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_i32                           0x9d0cdde00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_f32                           0x9d0cde100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_f16                           0x9d0cde400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x9d0cde700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x9d0cdea00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x9d0cded00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x9d0cdf000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x9d0cdf300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x9d0cdf600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm                               0x9d0cdf900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm_mul                           0x9d0cdfc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x9d0ce0000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_l2_norm                                0x9d0ce0300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_group_norm                             0x9d0ce0600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_norm                                   0x9d0ce0900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x9d0ce0c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x9d0ce0f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x9d0ce1200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x9d0ce1500 | th_max =  384 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x9d0ce1800 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x9d0ce1b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x9d0ce1e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x9d0ce2100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x9d0ce2400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x9d0ce2700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x9d0ce2a00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x9d0ce2d00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x9d0ce3000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x9d0ce3300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x9d0ce3600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x9d0ce3900 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x9d0ce3c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x9cfe00000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x9cfe00300 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x9cfe00600 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x9cfe00900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x9cfe00c00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x9cfe00f00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x9cfe01200 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x9cfe01500 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x9cfe01800 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x9cfe01b00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x9cfe01e00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x9cfe02100 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x9cfe02400 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x9cfe02700 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x9cfe02a00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x9cfe02d00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x9cfe03000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x9cfe03300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x9cfe03600 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x9cfe03900 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x9cfe03c00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x9d0ce4000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x9d0ce4300 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x9d0ce4600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x9d0ce4900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x9d0ce4c00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x9d0ce4f00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x9d0ce5200 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x9d0ce5500 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x9d0ce5800 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x9d0ce5b00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x9d0ce5e00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x9d0ce6100 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x9d0ce6400 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x9d0ce6700 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x9d0ce6a00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x9d0ce6d00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x9d0ce7000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x9d0ce7300 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x9d0ce7600 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x9d0ce7900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x9d0ce7c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x9d0ce8000 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x9d0ce8300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x9d0ce8600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x9d0ce8900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x9d0ce8c00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x9d0ce8f00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x9d0ce9200 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x9d0ce9500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x9d0ce9800 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x9d0ce9b00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x9d0ce9e00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x9d0cea100 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x9d0cea400 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x9d0cea700 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x9d0ceaa00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x9d0cead00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x9d0ceb000 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x9d0ceb300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x9d0ceb600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x9d0ceb900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x9d0cebc00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x9cfe04000 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x9cfe04300 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x9cfe04600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x9cfe04900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x9cfe04c00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x9cfe04f00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x9cfe05200 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x9cfe05500 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x9cfe05800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x9cfe05b00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x9cfe05e00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x9cfe06100 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x9cfe06400 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x9cfe06700 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x9cfe06a00 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x9cfe06d00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x9cfe07000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x9cfe07300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x9cfe07600 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x9cfe07900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x9cfe07c00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x9d0cec000 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x9d0cec300 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x9d0cec600 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x9d0cec900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x9d0cecc00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x9d0cecf00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x9d0ced200 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x9d0ced500 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x9d0ced800 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x9d0cedb00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x9d0cede00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x9d0cee100 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x9d0cee400 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x9d0cee700 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x9d0ceea00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x9d0ceed00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x9d0cef000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x9d0cef300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x9d0cef600 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x9d0cef900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x9d0cefc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x9cfe08000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x9cfe08300 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x9cfe08600 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x9cfe08900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x9cfe08c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x9cfe08f00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x9cfe09200 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x9cfe09500 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x9cfe09800 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x9cfe09b00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x9cfe09e00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x9cfe0a100 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x9cfe0a400 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x9cfe0a700 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x9cfe0aa00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x9cfe0ad00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x9cfe0b000 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x9cfe0b300 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x9cfe0b600 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x9cfe0b900 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x9cfe0bc00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x9d0cf0000 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x9d0cf0300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f32                          0x9d0cf0600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f16                          0x9d0cf0900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_multi_f32                         0x9d0cf0c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_multi_f16                         0x9d0cf0f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_vision_f32                        0x9d0cf1200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_vision_f16                        0x9d0cf1500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f32                          0x9d0cf1800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f16                          0x9d0cf1b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f16                             0x9d0cf1e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f32                             0x9d0cf2100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x9d0cf2400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x9d0cf2700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x9d0cf2a00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x9d0cf2d00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_upscale_f32                            0x9d0cf3000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_f32                                0x9d0cf3300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x9d0cf3600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x9d0cf3900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_arange_f32                             0x9d0cf3c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x9cfe0c000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x9cfe0c300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x9cfe0c600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x9cfe0c900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x9cfe0cc00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x9cfe0cf00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x9cfe0d200 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x9cfe0d500 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x9cfe0d800 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x9cfe0db00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x9cfe0de00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x9cfe0e100 | th_max =  384 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x9cfe0e400 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x9cfe0e700 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x9cfe0ea00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x9cfe0ed00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x9cfe0f000 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x9cfe0f300 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x9cfe0f600 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x9cfe0f900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x9cfe0fc00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x9cfe10000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x9cfe10300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x9cfe10600 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x9cfe10900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x9cfe10c00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x9cfe10f00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x9cfe11200 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x9cfe11500 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x9cfe11800 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x9cfe11b00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x9cfe11e00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x9cfe12100 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x9cfe12400 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x9cfe12700 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x9cfe12a00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x9cfe12d00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x9cfe13000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x9cfe13300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x9cfe13600 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x9cfe13900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x9cfe13c00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x9cfe14000 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x9cfe14300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x9cfe14600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x9cfe14900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x9cfe14c00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x9cfe14f00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x9cfe15200 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x9cfe15500 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x9cfe15800 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x9cfe15b00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x9cfe15e00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x9cfe16100 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x9cfe16400 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x9cfe16700 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x9cfe16a00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x9cfe16d00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x9cfe17000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x9cfe17300 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x9cfe17600 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x9cfe17900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x9cfe17c00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x9cfe18000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x9cfe18300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x9cfe18600 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x9cfe18900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x9cfe18c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x9cfe18f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x9cfe19200 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x9cfe19500 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x9cfe19800 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x9cfe19b00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x9cfe19e00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x9cfe1a100 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x9cfe1a400 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x9cfe1a700 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x9cfe1aa00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x9cfe1ad00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x9cfe1b000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x9cfe1b300 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x9cfe1b600 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x9cfe1b900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x9cfe1bc00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x9cfe1c000 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x9cfe1c300 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x9cfe1c600 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x9cfe1c900 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x9cfe1cc00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x9cfe1cf00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x9cfe1d200 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x9cfe1d500 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x9cfe1d800 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x9cfe1db00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x9cfe1de00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x9cfe1e100 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x9cfe1e400 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x9cfe1e700 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x9cfe1ea00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_f32                                0x9cfe1ed00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_i32                                0x9cfe1f000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x9cfe1f300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x9cfe1f600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x9cfe1f900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x9cfe1fc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x9d0cf4000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x9d0cf4300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x9d0cf4600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x9d0cf4900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x9d0cf4c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x9d0cf4f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x9d0cf5200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x9d0cf5500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x9d0cf5800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x9d0cf5b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x9d0cf5e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x9d0cf6100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x9d0cf6400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x9d0cf6700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x9d0cf6a00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x9d0cf6d00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_concat                                 0x9d0cf7000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqr                                    0x9d0cf7300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqrt                                   0x9d0cf7600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sin                                    0x9d0cf7900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cos                                    0x9d0cf7c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_neg                                    0x9d0cf8000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_reglu                                  0x9d0cf8300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_geglu                                  0x9d0cf8600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_swiglu                                 0x9d0cf8900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_swiglu_oai                             0x9d0cf8c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_geglu_erf                              0x9d0cf8f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_geglu_quick                            0x9d0cf9200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sum_rows                               0x9d0cf9500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mean                                   0x9d0cf9800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argmax                                 0x9d0cf9b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x9d0cf9e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x9d0cfa100 | th_max = 1024 | th_width =   32\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "create_memory: n_ctx = 512 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified: layer  28: dev = CPU\n",
            "llama_kv_cache_unified: layer  29: dev = CPU\n",
            "llama_kv_cache_unified: layer  30: dev = CPU\n",
            "llama_kv_cache_unified: layer  31: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   192.00 MiB\n",
            "llama_kv_cache_unified: size =  192.00 MiB (   512 cells,  32 layers,  1/1 seqs), K (f16):   96.00 MiB, V (f16):   96.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 3\n",
            "llama_context: max_nodes = 1560\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =    73.01 MiB\n",
            "llama_context: graph nodes  = 1126\n",
            "llama_context: graph splits = 322 (with bs=512), 1 (with bs=1)\n",
            "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.pre': 'default', 'general.file_type': '15', 'general.quantization_version': '2', 'phi3.rope.dimension_count': '96', 'tokenizer.ggml.bos_token_id': '1', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.attention.head_count_kv': '32', 'phi3.attention.head_count': '32', 'tokenizer.ggml.model': 'llama', 'phi3.block_count': '32', 'general.architecture': 'phi3', 'phi3.feed_forward_length': '8192', 'phi3.embedding_length': '3072', 'general.name': 'Phi3', 'phi3.context_length': '4096'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
            "' + message['content'] + '<|end|>' + '\n",
            "' + '<|assistant|>' + '\n",
            "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
            "'}}{% endif %}{% endfor %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <s>\n",
            "ggml_metal_free: deallocating\n",
            "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
            "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
            "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
            "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
            "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
            "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
            "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
            "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
            "llama_perf_context_print:        load time =     367.17 ms\n",
            "llama_perf_context_print: prompt eval time =     367.01 ms /    13 tokens (   28.23 ms per token,    35.42 tokens per second)\n",
            "llama_perf_context_print:        eval time =      69.37 ms /     2 runs   (   34.68 ms per token,    28.83 tokens per second)\n",
            "llama_perf_context_print:       total time =     437.17 ms /    15 tokens\n",
            "llama_perf_context_print:    graphs reused =          1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'llm': {'choice': 'Positive'}}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline.run(\n",
        "    {\n",
        "        \"prompt_builder\": {\"statement\": \"I love Italian food\"},\n",
        "        \"llm\": {\"generation_kwargs\": {\"temperature\": 0.5}},\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31e75e3f",
      "metadata": {},
      "source": [
        "## Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1176137b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from haystack import Pipeline\n",
        "from haystack.components.builders import PromptBuilder\n",
        "\n",
        "from outlines_haystack.generators.llama_cpp import LlamaCppTextGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fea4650",
      "metadata": {},
      "outputs": [],
      "source": [
        "generator = LlamaCppTextGenerator(\n",
        "    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
        "    file_name=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23d5d180",
      "metadata": {},
      "source": [
        "### Standalone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4b1e7adf",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_load_from_file_impl: using device Metal (Apple M1 Pro) - 25553 MiB free\n",
            "llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from /Users/edoardoabati/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct-gguf/snapshots/a64113399c2f6b8ad3e11c394733a2ddadaa7f33/./Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
            "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:   81 tensors\n",
            "llama_model_loader: - type q5_K:   32 tensors\n",
            "llama_model_loader: - type q6_K:   17 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 2.23 GiB (5.01 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control-looking token:  32007 '<|end|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
            "load: control-looking token:  32000 '<|endoftext|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: printing all EOG tokens:\n",
            "load:   - 32000 ('<|endoftext|>')\n",
            "load:   - 32007 ('<|end|>')\n",
            "load: special tokens cache size = 67\n",
            "load: token to piece cache size = 0.1690 MB\n",
            "print_info: arch             = phi3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 3072\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 96\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 96\n",
            "print_info: n_embd_head_v    = 96\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 3072\n",
            "print_info: n_embd_v_gqa     = 3072\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8192\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 3.82 B\n",
            "print_info: general.name     = Phi3\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32064\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 32000 '<|endoftext|>'\n",
            "print_info: EOT token        = 32007 '<|end|>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: PAD token        = 32000 '<|endoftext|>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 32000 '<|endoftext|>'\n",
            "print_info: EOG token        = 32007 '<|end|>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 194 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors: offloading 0 repeating layers to GPU\n",
            "load_tensors: offloaded 0/33 layers to GPU\n",
            "load_tensors:   CPU_Mapped model buffer size =  2281.66 MiB\n",
            "...........................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 512\n",
            "llama_context: n_ctx_per_seq = 512\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
            "ggml_metal_init: allocating\n",
            "ggml_metal_init: found device: Apple M1 Pro\n",
            "ggml_metal_init: picking default device: Apple M1 Pro\n",
            "ggml_metal_init: GPU name:   Apple M1 Pro\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
            "ggml_metal_init: GPU family: MTLGPUFamilyMetal4  (5002)\n",
            "ggml_metal_init: simdgroup reduction   = true\n",
            "ggml_metal_init: simdgroup matrix mul. = true\n",
            "ggml_metal_init: has residency sets    = true\n",
            "ggml_metal_init: has bfloat            = true\n",
            "ggml_metal_init: use bfloat            = false\n",
            "ggml_metal_init: hasUnifiedMemory      = true\n",
            "ggml_metal_init: recommendedMaxWorkingSetSize  = 26800.60 MB\n",
            "ggml_metal_init: loaded kernel_add                                    0x9d0cfa400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_2                             0x9d0cfa700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_3                             0x9d0cfaa00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_4                             0x9d0cfad00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_5                             0x9d0cfb000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_6                             0x9d0cfb300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_7                             0x9d0cfb600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_fuse_8                             0x9d0cfb900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4                             0x9d0cfbc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x9ce5e5200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x9ce5e5800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x9ce5e5500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x9ce5e4f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x9ce5e4c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x9ce5e4900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x9ce5e4600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub                                    0x9ce5e4300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sub_row_c4                             0x9ce5e4000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul                                    0x9ce51fc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_row_c4                             0x9ce51f900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div                                    0x9ce51f600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_div_row_c4                             0x9ce51f300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_add_id                                 0x9ce51f000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f32                             0x9ce51ed00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_f16                             0x9ce51ea00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i32                             0x9ce51e700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_repeat_i16                             0x9ce51e400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale                                  0x9ce51e100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_scale_4                                0x9ce51de00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_clamp                                  0x9ce51db00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_tanh                                   0x9ce51d800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_relu                                   0x9ce51d500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sigmoid                                0x9ce51d200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu                                   0x9ce51cf00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_4                                 0x9ce51cc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_erf                               0x9ce51c900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_erf_4                             0x9ce51c600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick                             0x9ce51c000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_gelu_quick_4                           0x9ce5c8000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu                                   0x9ce5c8300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_silu_4                                 0x9ce5c8600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_elu                                    0x9ce5c8900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_abs                                    0x9ce5c8c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sgn                                    0x9ce5c8f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_step                                   0x9ce5c9200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_hardswish                              0x9ce5c9500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_hardsigmoid                            0x9ce5c9800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_exp                                    0x9ce5c9b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16                           0x9ce5c9e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x9ce5ca100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32                           0x9ce5ca400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x9ce5ca700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf                          0x9ce5caa00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x9ce5cad00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f32                           0x9ce5cb000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_f16                           0x9ce5cb300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x9ce5cb600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x9ce5cb900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x9ce5cbc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x9ce3e0000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x9ce3e0300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x9ce3e0600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x9ce3e0900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x9ce3e0c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x9ce3e0f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x9ce3e1200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x9ce3e1500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x9ce3e1800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x9ce3e1b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x9ce3e1e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x9ce3e2100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x9ce3e2400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x9ce3e2700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x9ce3e2a00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x9ce3e2d00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x9ce3e3000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_get_rows_i32                           0x9ce3e3300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_f32                           0x9ce3e3600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_f16                           0x9ce3e3900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x9ce3e3c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x9ce594000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x9ce594300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x9ce594600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x9ce594900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x9ce594c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm                               0x9ce594f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm_mul                           0x9ce595200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x9ce595500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_l2_norm                                0x9ce595800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_group_norm                             0x9ce595b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_norm                                   0x9ce595e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x9ce596100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x9ce596400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x9ce596700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x9ce596a00 | th_max =  384 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x9ce596d00 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x9ce597000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x9ce597300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x9ce597600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x9ce597900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x9ce597c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x9ce3b8000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x9ce3b8300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x9ce3b8600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x9ce3b8900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x9ce3b8c00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x9ce3b8f00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x9ce3b9200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x9ce3b9500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x9ce3b9800 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x9ce3b9b00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x9ce3b9e00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x9ce3ba100 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x9ce3ba400 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x9ce3ba700 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x9ce3baa00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x9ce3bad00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x9ce3bb000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x9ce3bb300 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x9ce3bb600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x9ce3bb900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x9ce3bbc00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x9ce3a0000 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x9ce3a0300 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x9ce3a0600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x9ce3a0900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x9ce3a0c00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x9ce3a0f00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x9ce3a1200 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x9ce3a1500 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x9ce3a1800 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x9ce3a1b00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x9ce3a1e00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x9ce3a2100 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x9ce3a2400 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x9ce3a2700 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x9ce3a2a00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x9ce3a2d00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x9ce3a3000 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x9ce3a3300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x9ce3a3600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x9ce3a3900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x9ce3a3c00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x9ce380000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x9ce380300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x9ce380600 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x9ce380900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x9ce380c00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x9ce380f00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x9ce381200 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x9ce381500 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x9ce381800 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x9ce381b00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x9ce381e00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x9ce382100 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x9ce382400 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x9ce382700 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x9ce382a00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x9ce382d00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x9ce383000 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x9ce383300 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x9ce383600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x9ce383900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x9ce383c00 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x9ce374000 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x9ce374300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x9ce374600 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x9ce374900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x9ce374c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x9ce374f00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x9ce375200 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x9ce375500 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x9ce375800 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x9ce375b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x9ce375e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x9ce376100 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x9ce376400 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x9ce376700 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x9ce376a00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x9ce376d00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x9ce377000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x9ce377300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x9ce377600 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x9ce377900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x9ce377c00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x9ce448000 | th_max =  448 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x9ce448300 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x9ce448600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x9ce448900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x9ce448c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x9ce448f00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x9ce449200 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x9ce449500 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x9ce449800 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x9ce449b00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x9ce449e00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x9ce44a100 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x9ce44a400 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x9ce44a700 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x9ce44aa00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x9ce44ad00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x9ce44b000 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x9ce44b300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x9ce44b600 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x9ce44b900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x9ce44bc00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x9ce298000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x9ce298300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x9ce298600 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x9ce298900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x9ce298c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x9ce298f00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x9ce299200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x9ce299500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x9ce299800 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x9ce299b00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x9ce299e00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x9ce29a100 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x9ce29a400 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x9ce29a700 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x9ce29aa00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x9ce29ad00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x9ce29b000 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x9ce29b300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x9ce29b600 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x9ce29b900 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x9ce29bc00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x9ce2ac000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x9ce2ac300 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x9ce2ac600 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x9ce2ac900 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x9ce2acc00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x9ce2acf00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x9ce2ad200 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x9ce2ad500 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x9ce2ad800 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f32                          0x9ce2adb00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_norm_f16                          0x9ce2ade00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_multi_f32                         0x9ce2ae100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_multi_f16                         0x9ce2ae400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_vision_f32                        0x9ce2ae700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_vision_f16                        0x9ce2aea00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f32                          0x9ce2aed00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_rope_neox_f16                          0x9ce2af000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f16                             0x9ce2af300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_f32                             0x9ce2af600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x9ce2af900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x9ce2afc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x9ce494000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x9ce494300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_upscale_f32                            0x9ce494600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_f32                                0x9ce494900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x9ce494c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x9ce494f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_arange_f32                             0x9ce495200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x9ce495500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x9ce495800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x9ce495b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x9ce495e00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x9ce496100 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x9ce496400 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x9ce496700 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x9ce496a00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x9ce496d00 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x9ce497000 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x9ce497300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x9ce497600 | th_max =  384 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x9ce497900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x9ce497c00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x9ce4ac000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x9ce4ac300 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x9ce4ac600 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x9ce4ac900 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x9ce4acc00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x9ce4acf00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x9ce4ad200 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x9ce4ad500 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x9ce4ad800 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x9ce4adb00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x9ce4ade00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x9ce4ae100 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x9ce4ae400 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x9ce4ae700 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x9ce4aea00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x9ce4aed00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x9ce4af000 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x9ce4af300 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x9ce4af600 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x9ce4af900 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x9ce4afc00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x9ce2d8000 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x9ce2d8300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x9ce2d8600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x9ce2d8900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x9ce2d8c00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x9ce2d8f00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x9ce2d9200 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x9ce2d9500 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x9ce2d9800 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x9ce2d9b00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x9ce2d9e00 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x9ce2da100 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x9ce2da400 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x9ce2da700 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x9ce2daa00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x9ce2dad00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x9ce2db000 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x9ce2db300 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x9ce2db600 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x9ce2db900 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x9ce2dbc00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x9ce2f0000 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x9ce2f0300 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x9ce2f0600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x9ce2f0900 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x9ce2f0c00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x9ce2f0f00 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x9ce2f1200 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x9ce2f1500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x9ce2f1800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x9ce2f1b00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x9ce2f1e00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x9ce2f2100 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x9ce2f2400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x9ce2f2700 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x9ce2f2a00 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x9ce2f2d00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x9ce2f3000 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x9ce2f3300 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x9ce2f3600 | th_max =  896 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x9ce2f3900 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x9ce2f3c00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x9ce4e8000 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x9ce4e8300 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x9ce4e8600 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x9ce4e8900 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x9ce4e8c00 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x9ce4e8f00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x9ce4e9200 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x9ce4e9500 | th_max =  768 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x9ce4e9800 | th_max =  704 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x9ce4e9b00 | th_max =  832 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x9ce4e9e00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x9ce4ea100 | th_max =  640 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x9ce4ea400 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x9ce4ea700 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x9ce4eaa00 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x9ce4ead00 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x9ce4eb000 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x9ce4eb300 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x9ce4eb600 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x9ce4eb900 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x9ce4ebc00 | th_max =  512 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x9ce504000 | th_max =  576 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_f32                                0x9ce504300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_set_i32                                0x9ce504600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x9ce504900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x9ce504c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x9ce504f00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x9ce505200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x9ce505500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x9ce505800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x9ce505b00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x9ce505e00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x9ce506100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x9ce506400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x9ce506700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x9ce506a00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x9ce506d00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x9ce507000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x9ce507300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x9ce507600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x9ce507900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x9ce507c00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x9ce35c000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x9ce35c300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_concat                                 0x9ce35c600 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqr                                    0x9ce35c900 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sqrt                                   0x9ce35cc00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sin                                    0x9ce35cf00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_cos                                    0x9ce35d200 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_neg                                    0x9ce35d500 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_reglu                                  0x9ce35d800 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_geglu                                  0x9ce35db00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_swiglu                                 0x9ce35de00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_swiglu_oai                             0x9ce35e100 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_geglu_erf                              0x9ce35e400 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_geglu_quick                            0x9ce35e700 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_sum_rows                               0x9ce35ea00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_mean                                   0x9ce35ed00 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_argmax                                 0x9ce35f000 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x9ce35f300 | th_max = 1024 | th_width =   32\n",
            "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x9ce35f600 | th_max = 1024 | th_width =   32\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "create_memory: n_ctx = 512 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified: layer  28: dev = CPU\n",
            "llama_kv_cache_unified: layer  29: dev = CPU\n",
            "llama_kv_cache_unified: layer  30: dev = CPU\n",
            "llama_kv_cache_unified: layer  31: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   192.00 MiB\n",
            "llama_kv_cache_unified: size =  192.00 MiB (   512 cells,  32 layers,  1/1 seqs), K (f16):   96.00 MiB, V (f16):   96.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 3\n",
            "llama_context: max_nodes = 1560\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =    73.01 MiB\n",
            "llama_context: graph nodes  = 1126\n",
            "llama_context: graph splits = 322 (with bs=512), 1 (with bs=1)\n",
            "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.pre': 'default', 'general.file_type': '15', 'general.quantization_version': '2', 'phi3.rope.dimension_count': '96', 'tokenizer.ggml.bos_token_id': '1', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.attention.head_count_kv': '32', 'phi3.attention.head_count': '32', 'tokenizer.ggml.model': 'llama', 'phi3.block_count': '32', 'general.architecture': 'phi3', 'phi3.feed_forward_length': '8192', 'phi3.embedding_length': '3072', 'general.name': 'Phi3', 'phi3.context_length': '4096'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
            "' + message['content'] + '<|end|>' + '\n",
            "' + '<|assistant|>' + '\n",
            "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
            "'}}{% endif %}{% endfor %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <s>\n"
          ]
        }
      ],
      "source": [
        "generator.warm_up()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f264bcec",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =     334.41 ms\n",
            "llama_perf_context_print: prompt eval time =     292.41 ms /     8 tokens (   36.55 ms per token,    27.36 tokens per second)\n",
            "llama_perf_context_print:        eval time =     515.12 ms /    15 runs   (   34.34 ms per token,    29.12 tokens per second)\n",
            "llama_perf_context_print:       total time =     809.62 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =         15\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'replies': [\"\\n<|assistant|> The capital of Italy is Rome. It's a city rich in\"]}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator.run(\n",
        "    prompt=\"What is the capital of Italy?\",\n",
        "    generation_kwargs={\"temperature\": 0.5},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea13a8d7",
      "metadata": {},
      "source": [
        "### In a Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1e6db0d5",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:haystack.components.builders.prompt_builder:PromptBuilder has 1 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<haystack.core.pipeline.pipeline.Pipeline object at 0x1272d2990>\n",
              " Components\n",
              "  - prompt_builder: PromptBuilder\n",
              "  - llm: LlamaCppTextGenerator\n",
              " Connections\n",
              "  - prompt_builder.prompt -> llm.prompt (str)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_template = \"What is the capital of {{country}}?\"\n",
        "\n",
        "pipeline = Pipeline()\n",
        "pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
        "pipeline.add_component(\n",
        "    instance=LlamaCppTextGenerator(\n",
        "        repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
        "        file_name=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
        "    ),\n",
        "    name=\"llm\",\n",
        ")\n",
        "pipeline.connect(\"prompt_builder\", \"llm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "260badc1",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =     251.89 ms\n",
            "llama_perf_context_print: prompt eval time =     509.20 ms /     8 tokens (   63.65 ms per token,    15.71 tokens per second)\n",
            "llama_perf_context_print:        eval time =     516.20 ms /    15 runs   (   34.41 ms per token,    29.06 tokens per second)\n",
            "llama_perf_context_print:       total time =    1026.97 ms /    23 tokens\n",
            "llama_perf_context_print:    graphs reused =         15\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'llm': {'replies': ['\\n<|assistant|> The capital of France is Paris.\\n\\nThis question is straightforward and']}}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline.run(\n",
        "    {\n",
        "        \"prompt_builder\": {\"country\": \"France\"},\n",
        "        \"llm\": {\"generation_kwargs\": {\"temperature\": 0.5}},\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85eecac8",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.9)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
